# ðŸ§© JAX DQN on Jumanji's Game2048-v1

This repository demonstrates a **Deep Q-Network (DQN)** agent implemented entirely in **JAX**, trained on the [`Game2048-v1`](https://github.com/instadeepai/jumanji/tree/main/jumanji/environments/logic/game_2048) environment from **Jumanji**.

The project highlights how to combine:
- ðŸ§  **Jumanji** for clean, JIT-compatible environments  
- âš¡ **JAX** for high-performance functional ML  
- ðŸ’¾ **Flashbax** for efficient, device-friendly replay buffers  

All together, this forms a lightweight and elegant reinforcement learning pipeline.

---

## ðŸš€ Highlights

- ðŸ§  **DQN implementation in JAX** â€” fully differentiable and JIT-compiled  
- ðŸŽ® **Jumanjiâ€™s Game2048-v1** â€” a clean, RL-compatible 2048 environment  
- âš¡ **Fast & reproducible** â€” powered by JAX transformations (`jit`, `vmap`)  

---

---

## ðŸ§© Environment Details

- **Environment ID:** `Game2048-v1`  
- **Provided by:** [Jumanji](https://github.com/instadeepai/jumanji)  
- **Action Space:**  
  `0 = up`, `1 = right`, `2 = down`, `3 = left`  
- **Observation:**  
  - `board`: `int32[4, 4]` (stores exponents of 2, 0 = empty)  
  - `action_mask`: `bool[4]` (valid moves)  
  - `step_count`: `int32`  
- **Reward:** Sum of merged tile values after each valid move.

---

